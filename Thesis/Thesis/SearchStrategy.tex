
\chapter{Search Strategy}
\label{ch:SearchStrategy}
%Deep thoughts go here.
\section{Major Backgrounds}
\section{Event Reconstruction}
\section{Neural Network}
\label{sec:NN} 
To help distinguish signal events from the majority of background events a neural network was employeed for event classification.  Neural networks are multivariate methods that take a variety of inputs and output a number between 0 and 1.  The output value is a discriminating variable that will be used to classify events and determine which events make it into the final Signal Region selection.  Signal-like events accumulate towards 1 while background-like events cluster around 0.  Two neural networks are trained, one for the electron+jets final state and one for the muon+jets final state.  This section will discuss the neural network studies completed and their uses in the search for FCNC events.  

\subsection{Input Variables}
A wide variety of input variables to the neural network were studied in detail.  Studies were done using only low level variables such as the kinematic variables  ($p_T$, $\eta$, $\phi$, $E$)  of the physics objects in the signal region.  This was done as a complex enough neural network should be able to figure out useful high level/event level variables (i.e. invariant masses, geometric separations) but in practice a combination of some of these low level variables and high level variables used as inputs to the neural network proved to give the best separation and projected limits.  Using physical intuition to guide the neural network proved to be a valuable tool.

Combinations of 29 input variables were tested to start with however variables such as $\eta$ and $\phi$ tend to not have significant weights in the neural network and are left out in favor the the high level variables that include them (e.g., $\Delta R$ values).  A measure of how different the variables are between signal and background is the Separation.  Table \ref{tab:Separations} shows the separation values for the variables that are inputs to the final neural network.  Comparisons between the shapes of the input variables are shown in Figures \ref{fig:VarPlots1}, \ref{fig:VarPlots2}, and \ref{fig:VarPlots3}
\begin{table}[]
\begin{center}
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{ccc}
\hline
Variable  &  Separation e+jets   & Separation $\mu$+jets   \\  \hline 
$\gamma_{\text{iso}}$   &  41.18   &  18.70	\\   
$p_T (\gamma)$            &  24.07   & 23.14  	\\
$m_{q\gamma}$           &   28.27  &  22.73 	\\
$m_{l\gamma}$             &  7.56   &  8.53 	\\
$m_{bW} $                    &  11.60   & 11.02  	\\
$\Delta R_{j\gamma}$ &   5.64  &  4.60 	\\
$\Delta R_{b l}$            &   4.42  &  5.00 	\\
$m_{T}^{W}$              &   3.34  &  3.16 	\\
$S_T$                            &  3.30   &  3.83 	\\
$n_{\text{jets}}$         &  2.07   &  1.70 	\\
$\chi^{2}_{W}$           & 1.87    &  1.33 	\\
$p_T (q)$                      &  2.81    &  2.47	\\
$\Delta R_{l \gamma}$ &  1.17  &  1.40 	\\
E (lepton)                       &  0.87   & 0.85	\\	
$\slashed{E}_T  $          &  0.68   & 0.47 	\\
$p_T (b)$                       & 0.52    & 0.50	\\ \hline
\end{tabular}
}
\label{tab:Separations}
\caption{Separation of normalized variables between signal and bacground in the e+jets and $\mu$+jets channels for the variables used as input to the final neural network.  }
\end{center}
\end{table}

\[ \text{Separation} = \sum_{i}^{bins} \frac {n_{s i}-n_{b i}}{n_{s i}+n_{b i}}\]

Typically the kinematic variables with photon information have the biggest separation values.  This is expected because the signal photon comes directly from the decay of a top quark and is much more energetic than background photons.  Additional shape comparision plots for other investigated variables are shown in Appendix \ref{app:NN}. 

The neural networks are trained on MC events that have a chance of being in the signal region after basic event level cuts and optimized for signal significance.  Only events with 1 photon ($\>15 GeV$) and 1 bjet (MV2c10 77\% working point) are classified by the neural network.  The 77\% working point was chosen by training the neural network on events with only 1 bjet at each working point: 70\%, 77\%, and 85\% and picking the network and working point with the best estimated significance.  The b-tagging neural network study is shown in Section \ref{sec:btagNN}

\begin{figure}[h!]
\centering
\subfloat[$\gamma_{iso}$ topo$E_{T}$cone40]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/photon0_iso.png}}\hfil
\subfloat[$\gamma_{p_T}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/photon0_pt.png}}
\vspace{-4.5mm}
\subfloat[$m_{q \gamma}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/m_qgam.png}}\hfil
\subfloat[$m_{l \gamma}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/m_lgam.png}}   
\vspace{-4.5mm}
\subfloat[$m_{bW}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/m_tSM.png}}\hfil
\subfloat[$\Delta R_{j\gamma}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/deltaRjgam.png}}
\caption{Normalized variables showing the shapes of neural network input variables: $\gamma_{iso}$ topo$E_{T}$cone40, $\gamma_{p_T}$, $m_{q \gamma}$, $m_{l \gamma}$, $m_{bW}$, and $\Delta R_{j\gamma}$ }
\label{fig:VarPlots1}
\end{figure}



\begin{figure}[h!]
\centering
\subfloat[$\Delta R_{b l}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/deltaRbl.png}}\hfil
\subfloat[$m_{T}^{W}$ ]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/MWT.png}}
\vspace{-4.5mm}
\subfloat[$S_T$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/S_T.png}}\hfil
\subfloat[$n_{\text{jets}}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/njets.png}}   
\vspace{-4.5mm}
\subfloat[$\chi^{2}_{W}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/w_chi2.png}}\hfil
\subfloat[$p_T (q)$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/jet0_pt.png}}
\caption{Normalized variables showing the shapes of neural network input variables: $\Delta R_{b l}$, $m_{T}^{W}$ , $S_T$, $n_{\text{jets}}$, $\chi^{2}_{W}$, and $p_T (q)$}
\label{fig:VarPlots2}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[$\Delta R_{l \gamma}$]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/deltaRlgam.png}}\hfil
\subfloat[E (lepton)]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/lepton_e.png}}
\vspace{-4.5mm}
\subfloat[$\slashed{E}_T  $]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/met.png}}\hfil
\subfloat[$p_T (b)$ ]{\includegraphics[width=.4\columnwidth]{../ThesisImages/SearchStrategy/varplots/bjet0_pt.png}}   
\caption{Normalized variables showing the shapes of neural network input variables: $\Delta R_{l \gamma}$, E (lepton), $\slashed{E}_T  $, and $p_T (b)$}
\label{fig:VarPlots3}
\end{figure}


\subsection{Architecture}

A variety of architecures of dense neural networks are studied using \textsc{Keras}\cite{Keras} on top of the \textsc{TensorFlow} backend \cite{TensorFlow}.  Each network has a number of input nodes equal to the number of input variables.  Networks with one, two, and three hidden layers are investigated each with 20 nodes.  The output layer contains only a single node.  Every node in one layer is connected to every node in the next layer and the previous layer.  Every connection is assigned a weight that is optimized during the training of the network.  For every node in the network a value is computed using the weights and input values of the previous nodes using an activation function.  Nodes with the highest output of this function are more important to the fit.  The activation function used on the internal nodes in this search is the Rectified Linear Unit activation function.
\[ ReLU(x) = 
\begin{cases}
x, \qquad \text{if } x \geq 0\\
0, \qquad \text{if } x < 0
\end{cases}
\]
The output layer uses the sigmoid function, $\sigma(x)$, as an activation function.  The sigmoid function maps the output smoothly to the range (0,1).
\[ \sigma(x) = \frac{1}{1+e^{-x}}
\]
Every training step the weights of each node are updated following an optimization algorithm, in this case the \textsc{Adam} optimizer\cite{AdamOpt}.  This optimizer follows the steepest gradient to reach the minimum of the parameter of interest called the loss function.  The loss function used for these classification neural networks is the binary cross entropy:
\[\text{Loss} = -\frac{1}{N}\sum_{i=1}^{N}y_{i} \text{log}(p(y_{i}))+(1-y_{i})\text{log}(1-p(y_{i}))\]
where y is a binary indicator (0 or 1) if class label is the correct classification for observation and p is the predicted probability observation is the class label (0 or 1).  The logarithmic nature of this loss function means it applys small values to correctly assigned event but more harshly punishes mismatching of events.  Therefore having a similar number of signal and background events that get weighted similarly can improve the behavior of the network.  In rare decay searches typically the amount of signal events is significantly smaller than the amount of background events in the training sample.  Using the weight functionality in keras the total number of signal events can be scaled to be similar to the number of background events. 

Weighting the signal events this way allows the network to separate the signal and background events in a way that is significantly less harsh than without the weights by taking advantage of the loss function being used.  This improves the estimated significance of the neural network cut after the signal events are rescaled to their proper normalization values.  

\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{../ThesisImages/SearchStrategy/neural_net2.jpeg}
	\caption[Pictoral representation of neural network architecture with 3 input variables, 2 hidden layers with 4 nodes each, and 1 output layer.]{Pictoral representation of neural network architecture with 3 input variables, 2 hidden layers with 4 nodes each, and 1 output layer\cite{NNImage}.}
	\label{fig:NNArch}
\end{figure}

Various hyperparameters are used as inputs into the neural network as well as the optimizer used.  The \textsc{Adam} optimizer has a default learning rate of 0.001 which was not changed throughout these studies.   The learning rate corresponds to the amount that weights are updated during training.  A learning rate that is too large can mean the network never settles into a local minima as it is always missing the minima or at the very least it can take much longer to converge into a minima.  As the neural network training for this search always converged quickly and to a similar value after being tested multiple different times the learning rate was not adapted.  

Another hyperparameter of note is the batch size which defines the number of samples that are propagated through the network at once.  The batch size is of crucial importance in how long the training of the network takes.  A set of 1000 training samples with a batch size of 100 will propagate each set of 100 samples through the neural network every epoch, so 10 separate batches.  A larger batch size means that each epoch of the training takes a shorter amount of time.  However, as the weights are updated after each batch the network can take many more epochs to converge as the weights are being updated less frequently.  A batch size of 100 was used while training the networks presented in this chapter.  Larger batch sizes were tested with the only difference being the time each epoch took and the total time the network took to converge.

Epochs are the total number of times the network has been trained over the entire training set.  All of the networks were allowed up to 200 epochs to converge with a \textsc{keras} patience value set to 50.  The loss function minimization would be done every batch and after each epoch the best possible value of the loss function is found.  If this value is better than any previous epoch the network is allowed to train for 50 more epochs until 50 epochs have passed without finding a new minimum loss function value which then terminates the training.  All models converge early and are terminated typically between epoch 80 and 120 meaning the loss function was minimized between epoch 30 and 70.  

To avoid overtraining the network dropout regularization was used on each of the hidden layers.  Dropout has the effect of simulating a large number of networks with very different network structures by removing A dropout rate of 20\% was used meaning that for every batch 20\% of the weights of the hidden layer nodes were set to 0.  This forces the network to not become overly dependent on any given node and learning the data `by heart' as opposed to recognizing the trends in the sample. 

\[ z = \frac{x - m }{q_3 - q_1} \]

\[ \text{accuracy} = \frac{N(\text{event}_{NN} \geq 0.5|\text{signal})+ N(\text{event}_{NN} <0.5|\text{background})}{N(\text{signal})+N(\text{background})} \]
where $N(\text{event}_{NN} \geq 0.5|\text{signal})$ is the number of signal events with $P_{\text{signal}}\geq 0.5$.  Essentially the accuracy is a measure of the mean of how often correct prediction values occur assuming a cut on the output of $\geq0.5$.
% EJets Train Test Split
% train, test, val
% Sig: (72589, 47) (22685, 47) (18148, 47)
%ttbar (963721, 47) (301163, 47) (240931, 47)
%singleTop (56456, 47) (17643, 47) (14114, 47)
%ttV (190610, 47) (59566, 47) (47653, 47)
%diboson (68024, 47) (21258, 47) (17007, 47)
%WJets (195049, 47) (60953, 47) (48763, 47)
%ZJets (314462, 47) (98270, 47) (78616, 47)
%Mujets train Test Val
%Sig: (75607, 47) (23628, 47) (18902, 47)
%ttbar (912851, 47) (285266, 47) (228213, 47)
%singleTop (53772, 47) (16804, 47) (13444, 47)
%ttV (153174, 47) (47868, 47) (38294, 47)
%diboson (45536, 47) (14231, 47) (11384, 47)
%WJets (189872, 47) (59335, 47) (47468, 47)
%ZJets (104734, 47) (32730, 47) (26184, 47)
% 80% train, 20% test
% 80% newtrain, 20% val


Sci-ket Learn \cite{ScikitLearn}
\subsection{Hidden Layer Studies}
\subsection{B-Tagging Working Point Studies}
\label{sec:btagNN}

BTag WP selection
\section{Data and Simulation Event PreSelection}
\section{Control and Validation Regions}
\section{Signal Region}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%                                                                                                   %%%%%%%% 
%%%%%%%%%%%%                           BEN                                                                  %%%%%%%% 
%%%%%%%%%%%%                                                                                                   %%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%This chapter contains co-authored material from Ref.~\cite{Dijet2017}, written as part of the ATLAS Collaboration.
%\newline
%
%Compared to most ATLAS analyses, the search strategy is very straightforward:
%
%\begin{itemize}[noitemsep]
%	\item Event passes the lowest unprescaled trigger, HLT\_j380
%	\item Events are part of the Good Run List
%	\item Events are not flagged as having a calorimeter noise burst, or as having incomplete event data
%	\item Leading jet $\pt > 440$\,GeV
%	\item Subleading jet $\pt > 60$\,GeV
%	\item All jets with $\pt > 60$\,GeV pass jet cleaning cuts
%\end{itemize}
%
%To help discriminate between the $t$-channel dominated QCD background and any possible $s$-channel resonances from new physics processes, a cut is placed on the angular variable
%\begin{equation}
%y^* = \frac{y_1 + y_2}{2}
%\end{equation}
%where $y_1$ and $y_2$ are the rapidities of the leading and subleading jets, respectively.  The cut is chosen as it has the best discrimination between signal and background for the ensemble of benchmark models used, or for the single benchmark in the case of the $W^*$ selection.  Finally, a cut is placed on the invariant mass of the two leading jets to ensure that the trigger is fully efficient when paired with the $y^*$ cut.  The cuts for the resonance and $W^*$ resonance selections are:
%
%\begin{itemize}[noitemsep]
%	\item $|y^*| <$ 0.6 (1.2)
%	\item $\mjj >$ 1.1 (1.7)\,TeV
%\end{itemize}
%
%
%
%\section{Background Fit}
%Previous versions of the dijet resonance analysis, including those from CDF, ATLAS, and CMS, have fit the dijet invariant mass spectrum with a function of the form
%\begin{equation}
%f(z) = p_1(1-z)^{p_2}z^{p_3+p_4 ln(x)+p_5 ln(x)^2}
%\label{eq:dijet}
%\end{equation}
%where $z = m_{jj}/\sqrt{s}$ and the behavior is governed by the free parameters $p_i$.  Some past searches have required fewer terms in Eq.~\ref{eq:dijet}, such as by setting $p_4 = p_5 = 0$, but more parameters are required to properly describe the distribution as integrated luminosity increases.  Eq.~\ref{eq:dijet} was found to fit the observed spectrum in searches at CDF, and in ATLAS and CMS searches at both $\sqrt{s}$ = 8 and $\sqrt{s}$ = 13\,TeV.  The parameterization also fits the distribution obtained from simulated QCD samples.
%
%For the analysis of the full 2015+2016 ATLAS dataset, there was a worry that the aforementioned function would no longer be sufficient to properly describe the dataset.  While additional parameters could be added to Eq.~\ref{eq:dijet}, the function is already ad-hoc in nature, and any additional terms could possibly see diminishing returns in usefulness.  As such, this analysis was used to test and implement a new method for fitting the background, resulting in the use of the sliding-window fit, or SWiFt as the specific implementation was named.  The goal was to create a fitting method that was more robust against increases in luminosity while maintaining the same results in the regime where the global fit still held.
%
%\subsection{The Wilks' Statistical Test}
%
%To ensure that enough parameters are included in the fit function used to properly describe the observed spectrum, it must be determined that adding additional terms does not significantly improve the fit, and that any bumps in the observed spectrum do not come from under-fitting the data.  To determine the number of parameters needed, the Wilks' statistic is used.  When comparing lower and higher parameter versions of Eq.~\ref{eq:dijet}, the statistic reveals how much of an improvement an additional parameter in the fit function provides.  In the case of a p-value$<$0.05, the $n-1$ parameter function is discarded, and the statistic for $n$ vs. $n+1$ is calculated.  The evolution of the test statistics over time is shown in Figure~\ref{fig:Wilks}.  For the full 37\,fb$^{-1}$ dataset the 4-parameter function is preferred.  The decline in the viability of the 3-parameter function is clearly visible, becoming unviable once around 25\,fb$^{-1}$ of data is used.
%
%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=0.75\columnwidth]{figures/SearchStrategy/Wilks.png}
%	\caption{Wilks' test between the 4-parameter fit function and the 3/5-parameter versions.  For the full 2015+2016 dataset, the 4-parameter fit provides a much better description of the data than the 3-parameter fit, but the 5-parameter fit does not provide additional improvement.}
%	\label{fig:Wilks}
%\end{figure}
%
%For the full dataset, a global fit function with four parameters is still a viable fit.  (If a 5-parameter version was preferred, a 6th parameter would be needed to assess the goodness of fit, but such a term has not yet been determined)  This means that a direct comparison can be made between the sliding window fit and the global fit to ensure that the sliding window gives the same results.
%
%\subsection{The Sliding Window Fit (SWiFt)}
%
%The nominal size for the sliding window was chosen as the largest window which used one less parameter than the global fit while still proving a good fit to the function for all possible windows.  The three metrics used to measure this were the Wilks' statistic, chi-squared divided by the number of degrees of freedom, and the Kolmogorov-Smirnov (KS) test.  The results for various window sizes are show in Figure~\ref{fig:SwiftFitStats}.  All of the tested windows performed very well with the data, but the final chosen window uses 30 bins to the left of the center and 20 bins to the right, comprising approximately half of the total distribution, and larger than any of the tested signal shapes.  The asymmetry in the window allows for a stronger contribution from the higher-statistics, low-\mjj~bins in a given window to better anchor the fit as the window slides to the tail of the distribution.
%
%\begin{figure}[ht!]
%	\centering
%	\subfloat[]{\includegraphics[width=0.45\columnwidth]{figures/SearchStrategy/SwiftWilks.png}\label{subfig:SwiftWilks}}
%	\hspace{0.1\textwidth}%
%	\subfloat[]{\includegraphics[width=0.45\columnwidth]{figures/SearchStrategy/SwiftKS.png}\label{subfig:SwiftKS}}
%	\subfloat[]{\includegraphics[width=0.45\columnwidth]{figures/SearchStrategy/SwiftChi2.png}\label{subfig:SwiftChi2}}
%	\caption{Statistical measures used to determine the optimal window size for SWift. (a) Wilks' p-value comparing the nominal SWiFt 3-parameter fit with an alternate 4-parameter version (b) KS p-value comparing the fit to pseudodata (c) $\chi^2$/NDF comparing fit to pseudodata. }
%	\label{fig:SwiftFitStats}
%\end{figure}
%
%For each window, a fit is performed to the bins in the window using the three-parameter form of Eq.~\ref{eq:dijet}. The fit is used to give a background value at the bin "center" (keeping in mind that the window is asymmetric), and the center then slides one bin to the right and creates the next fit and returns a value for the next bin center.  This is done across the entire spectrum, including beyond the final filled bin in the data distribution.  The values for each center are then stitched together to create the full background distribution.
%
%To ensure the best fit across the whole spectrum, the sliding window changes shape at the two extremes of the distribution.  As it approaches the end of the distribution at low \mjj, the left side of the window is compressed down, shrinking to a smallest size of 10 bins to the left, 20 bins to the right.  This does not cause any loss in efficacy as the window fit is relatively insensitive to window size, as demonstrated by the 15\_20 window line in Figure~\ref{fig:SwiftFitStats}.  Below this point in the spectrum, the fit value for all bins below the window center is determined by their fit in this smallest window.  At the high-\mjj~end, the left side of the window is fixed once the window center reaches the 5.5\,TeV bin, and past that point the window expands in size.  This is done to maintain adequate statistics in the fit, especially as the window slides beyond the end of the data spectrum.  This system is demonstrated in Figure~\ref{fig:SwiftSlide}.
%
%\begin{figure}[]
%	\centering
%	\subfloat[]{\includegraphics[width=0.45\columnwidth]{figures/SearchStrategy/SwiftSlide1.pdf}\label{subfig:SwiftSlide1}}
%	\subfloat[]{\includegraphics[width=0.45\columnwidth]{figures/SearchStrategy/SwiftSlide2.pdf}\label{subfig:SwiftSlide2}}
%	\hspace{0.1\textwidth}%
%	\subfloat[]{\includegraphics[width=0.45\columnwidth]{figures/SearchStrategy/SwiftSlide3.pdf}\label{subfig:SwiftSlide3}}
%	\subfloat[]{\includegraphics[width=0.45\columnwidth]{figures/SearchStrategy/SwiftSlide4.pdf}\label{subfig:SwiftSlide4}}
%	\caption{An example of the bin-by-bin construction of the SWiFt background as seen in Ref.~\cite{SWiFt}.  The gray box shows the window considered at each point while the green line is the bin center for which the fit value is evaluated.  In (b) and (c) only the value at the bin center is used for the total background estimate, while in (a) and (d) the fit values before and after the center, respectively, are used as part of the overall background.}
%	\label{fig:SwiftSlide}
%\end{figure}
%
%Figure~\ref{fig:SwiftVsGlobal} shows a comparison of the background obtained from SWiFt with the 4-parameter global fit function.  Across the full spectrum no significant deviations are observed, and the limits obtained using the two different background fits are essentially identical.  For the paper results, only the SWiFt background results are shown to prevent any confusion between the two methods, and to encourage future use of the SWiFt method for similar searches.
%
%The full SWiFt method is now outlined in Ref.~\cite{SWiFt}, though it has evolved somewhat from its original implementation in the dijet search to its current form for use in future searches.
%
%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=0.75\columnwidth]{figures/SearchStrategy/SwiftVsGlobal.png}
%	\caption{Comparison of the global 4-parameter fit function to the SWiFt background using a 3-parameter fit in each window.  No significant variations between the two fits are observed.}
%	\label{fig:SwiftVsGlobal}
%\end{figure}
%
%\section{Search Phase}
%
%The search phase of the analysis deals with determining if there are any signs of new physics in the data set.  In contrast to the previous fitting section which sought to address whether or not a better description of the data was available through a more complex fit function, the search phase asks whether or not the obtained fit and the actual data spectrum are consistent with each other.  The dijet analysis uses BumpHunter to create its test statistic as it insensitive to the shape of any possible excess.
%
%\subsection{BumpHunter}
%
%Statistical tests such as the $\chi^2$ test measure the discrepancy between data and expectation in a bin-by-bin basis, but do not take into account the relation between bins.  For example, five consecutive bins with excesses over background is much more interesting for a physics search than if those bins had the same significance but alternated signs between bins.  To look for the "bumpiness" of data compared to prediction, BumpHunter scans over sets of adjacent bins and measures the significance if those bins were combined together into one.  The dijet analysis scans for all possible combinations of bins from 2 bins to half the full spectrum, much wider than any of the signal templates considered in the analysis and certain to cover any possible cases of interest.  (Since the bin widths are chosen to approximate the detector resolution, a resonance creating an excess in only one bin is exceedingly unlikely)  From this scan BumpHunter returns the most discrepant region, identified in this search as the region 4326â€“4595\,GeV.
%
%The global significance of the region is calculated by running BumpHunter over a set of 10,000 pseudo-experiments.  In each pseudo-experiment, a toy spectrum is created by Poisson-fluctuating each bin of the spectrum and then calcuating the BumpHunter test statistic for that toy.  From this set it is possible to calculate the portion of pseudo-experiments in which the observed data has an excess that is less significant.
%
%For this particular search an excess was not observed, but it is worth noting the procedure that was in place for such an eventuality.  In the case that BumpHunter returned a most discrepant region with a p-value $<$ 0.01, representing a very significant excess, the background fit would be re-derived using the sliding window method with all bins in the discrepant region removed from the fit, in turn removing any bias in the fit caused by the large fluctuation.  This new fit would then be used as the background estimate for the limit setting phase.